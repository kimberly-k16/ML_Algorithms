{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7274515a",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "In the previous notebook, we explored gradient descent, a fundamental optimization technique used to find the optimal parameters for linear regression models. Gradient descent iteratively adjusts model parameters to minimize the cost function, which measures the difference between the predicted values and the actual outcomes.\n",
    "\n",
    "While gradient descent is effective for fitting linear models, it does not inherently address the issue of overfitting. Overfitting occurs when a model learns not only the underlying pattern but also the noise in the training data, resulting in poor generalization to new data. To combat overfitting, we use regularization techniques that add a penalty to the cost function, discouraging overly complex models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c3a8e7",
   "metadata": {},
   "source": [
    "### 1.1. Ridge and Lasso Regression\n",
    "Ridge and Lasso regression are two such regularization techniques. They modify the linear regression cost function by introducing additional terms that penalize large coefficients. These techniques help to prevent overfitting by controlling the magnitude of the model parameters, leading to better generalization on unseen data. Note that we do not regularize the intercept term $\\theta_0$.\n",
    "\n",
    "- **Ridge Regression**: Also known as L2 regularization, Ridge regression adds a penalty proportional to the square of the magnitude of coefficients. This results in the following modified cost function:\n",
    "\n",
    "  $$\n",
    "  J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (f(x_i) - y_i)^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{n} \\theta_j^2\n",
    "  $$\n",
    "\n",
    "  Here, $ \\lambda $ is the regularization parameter that controls the strength of the penalty. Ridge regression tends to shrink all coefficients towards zero but generally does not set any coefficients exactly to zero.\n",
    "\n",
    "- **Lasso Regression**: Also known as L1 regularization, Lasso regression adds a penalty proportional to the absolute value of the coefficients. The cost function for Lasso regression is:\n",
    "\n",
    "  $$\n",
    "  J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (f(x_i) - y_i)^2 + \\lambda \\sum_{j=1}^{n} |\\theta_j|\n",
    "  $$\n",
    "\n",
    "  In this case, $ \\lambda $ controls the penalty strength. Lasso regression can drive some coefficients exactly to zero, performing feature selection by effectively ignoring some features.\n",
    "\n",
    "By implementing Ridge and Lasso regression from scratch, we will see how these regularization techniques influence the model's coefficients and performance. We'll use the Diabetes dataset from `sklearn.datasets` to compare the results of Ridge and Lasso regression. This will help us understand how regularization affects the complexity and interpretability of linear models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0347c37",
   "metadata": {},
   "source": [
    "# 2. Implementation\n",
    "We will now extend the `gradientDescentVec` function from the previous notebook. Note that we will only showcase the vectorized implementation of Ridge and Lasso Regression as it is computationally more feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "311b1001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentVec(X, y, alpha, iterations, lambda_reg=0, regularization=0):\n",
    "    \"\"\"\n",
    "    Perform gradient descent for linear regression with optional Ridge or Lasso regularization.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input feature matrix\n",
    "    - y: Target vector\n",
    "    - alpha: Learning rate\n",
    "    - iterations: Number of iterations for gradient descent\n",
    "    - lambda_reg: Regularization parameter (for Ridge or Lasso)\n",
    "    - regularization: Type of regularization (0 for none, 1 for Lasso, 2 for Ridge)\n",
    "\n",
    "    Returns:\n",
    "    - theta: Optimized parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add a column of ones to the input matrix X for the bias term (theta_0)\n",
    "    app_X = np.ones((X.shape[0], 1))\n",
    "    X = np.hstack((app_X, X))\n",
    "    \n",
    "    # Initialize the parameter vector theta with zeros\n",
    "    theta = np.zeros((X.shape[1], 1))\n",
    "    \n",
    "    # Perform gradient descent for the specified number of iterations\n",
    "    for i in range(iterations):\n",
    "        # Compute the predictions\n",
    "        predictions = np.dot(X, theta)\n",
    "        errors = predictions - y\n",
    "        \n",
    "        if regularization == 1:\n",
    "            # Lasso regularization\n",
    "            gradients = (1/X.shape[0]) * np.dot(X.T, errors) + lambda_reg * np.sign(theta)\n",
    "        elif regularization == 2:\n",
    "            # Ridge regularization\n",
    "            gradients = (1/X.shape[0]) * np.dot(X.T, errors) + (lambda_reg / X.shape[0]) * np.concatenate(([0], theta[1:].flatten()), axis=0).reshape(-1, 1)\n",
    "        else:\n",
    "            # No regularization\n",
    "            gradients = (1/X.shape[0]) * np.dot(X.T, errors)\n",
    "        \n",
    "        # Update theta using the gradient descent formula\n",
    "        theta -= alpha * gradients\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be68073e",
   "metadata": {},
   "source": [
    "Additional explanations to the implementation of the regularizations:\n",
    "\n",
    "- **Lasso**:\n",
    "     - `(1/X.shape[0])`: This factor normalizes the gradient by dividing by the number of samples `(X.shape[0])`, averaging the gradient over all training examples\n",
    "     - `np.dot(X.T, errors)`: This term calculates the gradient of the cost function with respect to the parameters theta without regularization. Here, `X.T` is the transpose of the feature matrix `X`, and `errors` is the vector of differences between the predicted values and the actual target values.\n",
    "     \n",
    "- **Ridge**:\n",
    "    - `lambda_reg / X.shape[0]`: Scales the regularization term by dividing by the number of samples, thus averaging the penalty over all training examples.\n",
    "    - `np.concatenate(([0], theta[1:]), axis=0)`: Creates a vector where the first element is 0 (for the intercept term) and the rest are the coefficients of theta starting from the second element. This ensures that the intercept is not regularized, while the other coefficients are penalized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50ba6e2",
   "metadata": {},
   "source": [
    "# 3. Comparison on Example Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded116f1",
   "metadata": {},
   "source": [
    "We will now use the Diabetes dataset to compare the theta matrices that unregularized, L1-regularized, and L2-regularized gradient descent produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82b66798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta with No Regularization:\n",
      " [[152.12691637]\n",
      " [ -0.31310953]\n",
      " [-11.25407205]\n",
      " [ 25.16330292]\n",
      " [ 15.32275063]\n",
      " [ -4.43042964]\n",
      " [ -4.24852822]\n",
      " [ -9.43327635]\n",
      " [  5.25839603]\n",
      " [ 23.01461081]\n",
      " [  3.35416841]]\n",
      "\n",
      "Theta with Ridge Regularization:\n",
      " [[152.12691637]\n",
      " [ -0.30437793]\n",
      " [-11.22269819]\n",
      " [ 25.12210248]\n",
      " [ 15.30138751]\n",
      " [ -4.39374972]\n",
      " [ -4.25247927]\n",
      " [ -9.43217031]\n",
      " [  5.266604  ]\n",
      " [ 22.95904187]\n",
      " [  3.37400217]]\n",
      "\n",
      "Theta with Lasso Regularization:\n",
      " [[ 1.51226956e+02]\n",
      " [-6.07332225e-04]\n",
      " [-9.54719735e+00]\n",
      " [ 2.49888211e+01]\n",
      " [ 1.43133383e+01]\n",
      " [-2.76998959e+00]\n",
      " [-2.80660436e+00]\n",
      " [-1.02610432e+01]\n",
      " [ 2.23401857e+00]\n",
      " [ 2.27969316e+01]\n",
      " [ 2.64133785e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and preprocess the diabetes dataset\n",
    "data = load_diabetes()\n",
    "X, y = data.data, data.target.reshape(-1, 1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Set parameters for gradient descent\n",
    "alpha = 0.01  # Learning rate\n",
    "iterations = 1000  # Number of iterations\n",
    "lambda_reg = 0.9  # Regularization parameter - high value selected for illustration\n",
    "\n",
    "# Perform gradient descent with no regularization\n",
    "theta_no_reg = gradientDescentVec(X, y, alpha, iterations, lambda_reg=0, regularization=0)\n",
    "\n",
    "# Perform gradient descent with Ridge regularization\n",
    "theta_ridge = gradientDescentVec(X, y, alpha, iterations, lambda_reg=lambda_reg, regularization=2)\n",
    "\n",
    "# Perform gradient descent with Lasso regularization\n",
    "theta_lasso = gradientDescentVec(X, y, alpha, iterations, lambda_reg=lambda_reg, regularization=1)\n",
    "\n",
    "print(\"Theta with No Regularization:\\n\", theta_no_reg)\n",
    "print(\"\\nTheta with Ridge Regularization:\\n\", theta_ridge)\n",
    "print(\"\\nTheta with Lasso Regularization:\\n\", theta_lasso)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
