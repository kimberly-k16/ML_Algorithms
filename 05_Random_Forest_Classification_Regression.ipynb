{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "755cfd12",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "In this notebook, we will implement a Random Forest classifier and regressor from scratch using the `DecisionTree` and `DecisionTreeRegressor` classes. Random Forest is an ensemble learning technique that combines multiple decision trees to improve prediction accuracy and robustness. It consists of multiple decision trees which were fitted on different subsets of the training data and features. Each tree in the forest provides a vote for classification or a prediction for regression, and the final output is determined by aggregating these votes or predictions. First of all, we will import the functionalities and classes from the Notebook on Decision Trees which are all saved in the `Decision_Trees.py` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f68dc7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Decision_Trees import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5d4416",
   "metadata": {},
   "source": [
    "### 1.1. Key Concepts of the Random Forest\n",
    "1. **Ensemble Learning**: Random Forest leverages the principle of ensemble learning, which involves combining multiple models to produce a stronger overall model. Instead of relying on a single decision tree, Random Forest constructs a collection of trees, each trained on different data samples and subsets of features. This approach helps in reducing overfitting and enhancing generalization.\n",
    "\n",
    "\n",
    "2. **Bootstrapping and Bagging**: Bootstrapping refers to the process of creating multiple subsets of the training data by sampling with replacement. Each decision tree in the Random Forest is trained on a different bootstrap sample, which ensures that the trees are diverse. Bagging (Bootstrap Aggregating) is used to combine the predictions of these trees to make the final decision, reducing the variance of the model and improving its stability.\n",
    "\n",
    "\n",
    "3. **Feature Subsampling**: During the training of each decision tree, a random subset of features is selected for splitting nodes. This technique, known as feature subsampling, introduces additional diversity among the trees and prevents them from becoming too similar. As a result, the ensemble of trees becomes more robust and less prone to overfitting.\n",
    "\n",
    "\n",
    "4. **Voting and Averaging**:\n",
    "   - **Classification**: For classification tasks, each tree in the forest votes for a class label. The final class prediction is determined by majority voting, where the class with the most votes is selected as the output.\n",
    "   - **Regression**: For regression tasks, each tree provides a continuous prediction. The final output is computed as the average of all individual tree predictions, which helps in smoothing out the effects of noisy data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1a196f",
   "metadata": {},
   "source": [
    "### 1.2. Advantages of Random Forests\n",
    "\n",
    "\n",
    "- **Reduced Overfitting**: By averaging multiple decision trees, Random Forest reduces the risk of overfitting that is common with single decision trees.\n",
    "\n",
    "\n",
    "- **Improved Accuracy**: Combining predictions from multiple trees often results in higher accuracy and better generalization to unseen data.\n",
    "\n",
    "\n",
    "- **Feature Importance**: Random Forest can evaluate the importance of different features, providing insights into which features contribute most to the predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b94624e",
   "metadata": {},
   "source": [
    "# 2. Implementation\n",
    "Let's start by implementing the Random Forest class for both classification and regression. This class will build on the `DecisionTree` and `DecisionTreeRegressor` classes and will include methods for fitting the forest to the data and making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79d05665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=100, max_depth=10, min_samples_split=2, max_features=None, regression=False):\n",
    "        \"\"\"\n",
    "        Initialize the Random Forest model.\n",
    "        \n",
    "        Parameters:\n",
    "        - n_trees (int): Number of decision trees in the forest.\n",
    "        - max_depth (int): Maximum depth of each decision tree.\n",
    "        - min_samples_split (int): Minimum number of samples required to split an internal node.\n",
    "        - max_features (int or None): Number of features to consider when looking for the best split.\n",
    "        - regression (bool): If True, use decision trees for regression; otherwise, use for classification.\n",
    "        \"\"\"\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.regression = regression\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Random Forest model to the training data.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (array-like, shape (n_samples, n_features)): Training feature matrix.\n",
    "        - y (array-like, shape (n_samples,)): Target values.\n",
    "        \"\"\"\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            # Initialize a new decision tree\n",
    "            if self.regression:\n",
    "                tree = DecisionTreeRegressor(\n",
    "                    min_samples_split=self.min_samples_split,\n",
    "                    max_depth=self.max_depth,\n",
    "                    max_features=self.max_features\n",
    "                )\n",
    "            else:\n",
    "                tree = DecisionTree(\n",
    "                    min_samples_split=self.min_samples_split,\n",
    "                    max_depth=self.max_depth,\n",
    "                    max_features=self.max_features\n",
    "                )\n",
    "            \n",
    "            # Create a bootstrap sample from the training data\n",
    "            bootstrap_indices = np.random.randint(0, X.shape[0], X.shape[0])\n",
    "            X_bootstrap, y_bootstrap = X[bootstrap_indices], y[bootstrap_indices]\n",
    "            \n",
    "            # Fit the decision tree on the bootstrap sample\n",
    "            tree.fit(X_bootstrap, y_bootstrap)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the target values for the given input data.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (array-like, shape (n_samples, n_features)): Feature matrix for prediction.\n",
    "        \n",
    "        Returns:\n",
    "        - array, shape (n_samples,): Predicted target values.\n",
    "        \"\"\"\n",
    "        if self.regression:\n",
    "            # For regression, aggregate predictions by averaging\n",
    "            predictions = np.zeros((self.n_trees, X.shape[0]))\n",
    "            for i, tree in enumerate(self.trees):\n",
    "                predictions[i] = tree.predict(X)\n",
    "            return np.mean(predictions, axis=0)\n",
    "        else:\n",
    "            # For classification, aggregate predictions by majority voting\n",
    "            predictions = np.zeros((self.n_trees, X.shape[0]), dtype=int)\n",
    "            for i, tree in enumerate(self.trees):\n",
    "                predictions[i] = tree.predict(X)\n",
    "            # Majority voting: select the most common class label for each sample\n",
    "            return np.array([np.bincount(pred).argmax() for pred in predictions.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d49f8b",
   "metadata": {},
   "source": [
    "### 2.1. Example Classification\n",
    "We will create a synthetic dataset to test how well our Random Forest Classifier performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e0c689b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a synthetic classification dataset\n",
    "X, y = make_classification(n_samples=500, n_features=20, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest Classifier\n",
    "rf_classifier = RandomForest(n_trees=10, max_depth=5, min_samples_split=4, max_features=10, regression=False)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f'Classification Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb1c42a",
   "metadata": {},
   "source": [
    "### 2.2. Example Regression\n",
    "We will use the California Housing dataset again in order to test how well our Random Forest Regressor predicts housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19652d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Mean Squared Error (MSE) on California Housing: 0.35\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the California Housing dataset\n",
    "california_data = fetch_california_housing()\n",
    "X, y = california_data.data, california_data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest Regressor\n",
    "rf_regressor = RandomForest(n_trees=10, max_depth=8, min_samples_split=4, max_features=5, regression=True)\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate using Mean Squared Error (MSE)\n",
    "mse = np.mean((y_pred - y_test) ** 2)\n",
    "print(f'Regression Mean Squared Error (MSE) on California Housing: {mse:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
