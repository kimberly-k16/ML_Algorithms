{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c4d5ca",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "In this notebook, we will explore how to implement a simple Dense Neural Network (DNN) from scratch using only basic Python and NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b5aab9",
   "metadata": {},
   "source": [
    "### 1.1. Theoretical and Mathematical Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43842cd1",
   "metadata": {},
   "source": [
    "#### 1.1.1. Structure of a Dense Neural Network (DNN)\n",
    "\n",
    "A Dense Neural Network (DNN) consists of **layers of neurons** where each neuron in one layer is connected to every neuron in the next layer. The main components of a DNN are:\n",
    "\n",
    "- **Input Layer**: Takes the features of the dataset (e.g., pixel values of an image) as input.\n",
    "- **Hidden Layers**: Intermediate layers where neurons compute activations through a weighted sum of inputs and pass them to an activation function.\n",
    "- **Output Layer**: Produces the final prediction, e.g., classification into categories.\n",
    "\n",
    "Each neuron in the network performs a simple operation: it computes a weighted sum of its inputs and passes the result through a nonlinear activation function.\n",
    "\n",
    "Mathematically, each neuron performs the following operation:\n",
    "\n",
    "$$\n",
    "Z^{[l]} = W^{[l]} A^{[l-1]} + B^{[l]}\n",
    "$$\n",
    "$$\n",
    "A^{[l]} = \\sigma(Z^{[l]})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ W^{[l]} $ is the weight matrix of the layer $ l $.\n",
    "- $ B^{[l]} $ is the bias vector of the layer $ l $.\n",
    "- $ A^{[l-1]} $ is the output (activation) of the previous layer.\n",
    "- $ Z^{[l]} $ is the linear component (weighted sum of inputs).\n",
    "- $ A^{[l]} $ is the activation (output) of layer $ l $ after applying the activation function $ \\sigma $.\n",
    "\n",
    "The activation function $ \\sigma $ introduces non-linearity into the network, allowing it to model complex relationships. Common activation functions are:\n",
    "\n",
    "- **Sigmoid**: $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $ (used in binary classification problems).\n",
    "- **ReLU** (Rectified Linear Unit): $ \\sigma(z) = \\max(0, z) $ (commonly used in hidden layers).\n",
    "\n",
    "#### 1.1.2. Forward Propagation\n",
    "\n",
    "Forward propagation is the process by which input data passes through the network to produce an output. It happens layer by layer, starting from the input layer and moving towards the output.\n",
    "\n",
    "In a single layer:\n",
    "\n",
    "1. The inputs from the previous layer (or from the dataset, in the case of the input layer) are multiplied by the weights of the current layer.\n",
    "2. A bias is added.\n",
    "3. The resulting weighted sum is passed through an activation function to produce the output (or activation) of the layer.\n",
    "\n",
    "The forward pass through the entire network can be described as the previous mentioned operations:\n",
    "\n",
    "$$\n",
    "Z^{[l]} = W^{[l]} A^{[l-1]} + B^{[l]}\n",
    "$$\n",
    "$$\n",
    "A^{[l]} = \\sigma(Z^{[l]})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ A^{[0]} = X $ (the input features).\n",
    "- $ A^{[L]} $ (where $ L $ is the total number of layers) represents the final output of the network.\n",
    "\n",
    "#### 1.1.3. Loss Function and Cost Function\n",
    "\n",
    "In machine learning, the loss function measures how well the network’s predictions match the actual labels. For a binary classification problem, the **binary cross-entropy loss** is commonly used:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(y, \\hat{y}) = - \\left( y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ y $ is the true label (0 or 1).\n",
    "- $ \\hat{y} $ is the predicted probability from the model (between 0 and 1).\n",
    "\n",
    "The **cost function** is the average of the loss over all examples in the dataset:\n",
    "\n",
    "$$\n",
    "J(W, B) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}(y^{(i)}, \\hat{y}^{(i)})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ m $ is the number of training examples.\n",
    "- $ y^{(i)} $ is the true label for the $ i $-th training example.\n",
    "- $ \\hat{y}^{(i)} $ is the network’s predicted output for the $ i $-th training example.\n",
    "\n",
    "#### 1.1.4. Backpropagation\n",
    "\n",
    "Backpropagation is the algorithm used to compute the gradients of the cost function with respect to the weights and biases. These gradients are then used to update the parameters via gradient descent.\n",
    "\n",
    "The key idea of backpropagation is to propagate the error from the output layer back through the network, calculating how much each weight contributed to the error. This is done using the **chain rule of calculus**.\n",
    "\n",
    "##### Backpropagation Steps:\n",
    "\n",
    "1. **Calculate the error at the output layer** (for classification, this is the difference between the predicted output and the true label):\n",
    "   $$\n",
    "   \\delta^{[L]} = A^{[L]} - Y\n",
    "   $$\n",
    "\n",
    "2. **Propagate the error backward** to calculate the error at each hidden layer:\n",
    "   $$\n",
    "   \\delta^{[l]} = \\left( W^{[l+1]} \\right)^T \\delta^{[l+1]} \\cdot \\sigma'(Z^{[l]})\n",
    "   $$\n",
    "   Where $ \\sigma'(Z^{[l]}) $ is the derivative of the activation function for layer $ l $.\n",
    "\n",
    "3. **Compute the gradients** for the weights and biases:\n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial W^{[l]}} = \\frac{1}{m} \\delta^{[l]} (A^{[l-1]})^T\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial B^{[l]}} = \\frac{1}{m} \\sum_{i=1}^{m} \\delta^{[l]}\n",
    "   $$\n",
    "\n",
    "The term $ \\delta^{[l]} $ is often referred to as the **error term** or **delta** for layer $ l $, and it captures how much the neurons in that layer contributed to the final error.\n",
    "\n",
    "#### 1.1.5. Gradient Descent\n",
    "\n",
    "Once we have the gradients for each layer, we use **gradient descent** to update the weights and biases. The goal of gradient descent is to minimize the cost function $ J(W, B) $.\n",
    "\n",
    "The update rule for gradient descent is:\n",
    "\n",
    "$$\n",
    "W^{[l]} := W^{[l]} - \\alpha \\frac{\\partial J}{\\partial W^{[l]}}\n",
    "$$\n",
    "$$\n",
    "B^{[l]} := B^{[l]} - \\alpha \\frac{\\partial J}{\\partial B^{[l]}}\n",
    "$$\n",
    "\n",
    "Where $ \\alpha $ is the **learning rate**, which controls how large the steps are that we take toward minimizing the cost function. A smaller learning rate may take longer to converge but provides more stable learning, while a larger learning rate can speed up training but risks overshooting the minimum.\n",
    "\n",
    "##### Steps in Gradient Descent:\n",
    "\n",
    "1. **Initialize the weights** $ W $ and biases $ B $ randomly, often small values close to zero.\n",
    "2. **Compute the forward pass** to get predictions $ A^{[L]} $.\n",
    "3. **Calculate the loss** using the predictions and true labels.\n",
    "4. **Backpropagate the error** to compute the gradients $ \\frac{\\partial J}{\\partial W^{[l]}} $ and $ \\frac{\\partial J}{\\partial B^{[l]}} $.\n",
    "5. **Update the weights and biases** using the computed gradients.\n",
    "\n",
    "Repeat the process for a certain number of **epochs** or until the cost function converges (i.e., the changes in the cost become negligibly small).\n",
    "\n",
    "#### 1.1.6. Putting it All Together\n",
    "\n",
    "Each training iteration consists of:\n",
    "1. **Forward propagation** to compute the output of the network.\n",
    "2. **Loss calculation** using the predicted output and true labels.\n",
    "3. **Backpropagation** to compute the gradients.\n",
    "4. **Gradient descent** to update the weights and biases.\n",
    "\n",
    "After many iterations, the network should \"learn\" to make good predictions by minimizing the cost function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa69295",
   "metadata": {},
   "source": [
    "# 2. Implementation\n",
    "Now let us write a DNN class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c633180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DenseNeuralNetwork:\n",
    "    \n",
    "    def __init__(self, layers:list, learning_rate:float, epochs:int):\n",
    "        \"\"\"\n",
    "        layers: List of layers including the input layer size, hidden layers sizes, and output layer size\n",
    "        learning_rate: Learning rate for gradient descent\n",
    "        epochs: Number of epochs to train the network\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def initialize_layers(self, X_rows:int):\n",
    "        \"\"\"\n",
    "        Initialize weights and biases for each layer\n",
    "        \"\"\"\n",
    "        # Input layer size followed by hidden layers and output layer\n",
    "        self.layers = [X_rows, *self.layers]\n",
    "        \n",
    "        # Biases and weights initialization\n",
    "        self.B = [np.random.randn(layer, 1)*0.01 for layer in self.layers[1:]]\n",
    "        self.W = [np.random.randn(layer, next_layer)*0.01 for \n",
    "                  layer, next_layer in zip(self.layers[:-1], self.layers[1:])]\n",
    "\n",
    "    def forward_propagation(self, X:np.array):\n",
    "        \"\"\"\n",
    "        Perform forward propagation through the network\n",
    "        \"\"\"\n",
    "        Z = []\n",
    "        A = []\n",
    "        \n",
    "        for B, W in zip(self.B, self.W):\n",
    "            # First layer uses input X\n",
    "            temp_z = np.dot(W.T, A[-1]) + B if Z else np.dot(W.T, X) + B\n",
    "            temp_a = self._sigmoid(temp_z)\n",
    "            Z.append(temp_z)\n",
    "            A.append(temp_a)\n",
    "            \n",
    "        return Z, A\n",
    "    \n",
    "    def backward_propagation(self, Z:list, A:list, y:np.array, X:np.array):\n",
    "        \"\"\"\n",
    "        Perform backpropagation and compute gradients for weights and biases\n",
    "        \"\"\"\n",
    "        dB = [np.zeros(B.shape) for B in self.B]\n",
    "        dW = [np.zeros(W.shape) for W in self.W]\n",
    "        \n",
    "        last_layer = len(self.layers)-2\n",
    "        deltas = []\n",
    "        \n",
    "        for layer in range(last_layer, -1, -1):\n",
    "            delta = (self._dsigmoid(Z[layer]) * np.dot(self.W[layer+1], deltas[-1]) if layer != last_layer \n",
    "                     else (A[layer]-y) * self._dsigmoid(Z[layer]))\n",
    "            deltas.append(delta)\n",
    "            dB[layer] = delta\n",
    "            dW[layer] = np.dot(A[layer-1], delta.T) if layer != 0 else np.dot(X, delta.T)\n",
    "            \n",
    "        return dB, dW\n",
    "            \n",
    "    def gradient_descent(self, X:np.array, y:np.array):\n",
    "        \"\"\"\n",
    "        Update weights and biases using gradient descent\n",
    "        \"\"\"\n",
    "        Z, A = self.forward_propagation(X)\n",
    "        \n",
    "        dBs, dWs = self.backward_propagation(Z, A, y, X)\n",
    "        self.B = [b - self.learning_rate*np.mean(dB, axis=1, keepdims=True) for b, dB in zip(self.B, dBs)]\n",
    "        self.W = [w - self.learning_rate*dW for w, dW in zip(self.W, dWs)]\n",
    "        \n",
    "    def fit(self, X:np.array, y:np.array):\n",
    "        \"\"\"\n",
    "        Train the neural network on the dataset\n",
    "        \"\"\"\n",
    "        self.initialize_layers(X.shape[0])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            self.gradient_descent(X, y)\n",
    "            \n",
    "    def predict(self, X:np.array):\n",
    "        \"\"\"\n",
    "        Make predictions on new data\n",
    "        \"\"\"\n",
    "        _, A = self.forward_propagation(X)\n",
    "        return np.argmax(A[-1], axis=0)\n",
    "    \n",
    "    def _sigmoid(self, z:np.array):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function\n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def _dsigmoid(self, z:np.array):\n",
    "        \"\"\"\n",
    "        Derivative of the sigmoid activation function\n",
    "        \"\"\"\n",
    "        return self._sigmoid(z) * (1 - self._sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a98f0e8",
   "metadata": {},
   "source": [
    "### 2.1. Example  Classification\n",
    "We will now train the network on a simple classification task with the Wine dataset from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "119c7f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 77.78%\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the Wine dataset from sklearn\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target.reshape(-1, 1)  # Reshape to a column vector\n",
    "\n",
    "# Normalize the data (feature scaling)\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)  # Standardize features to have mean=0 and variance=1\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Dense Neural Network\n",
    "layers = [13, 20, 10, 3]  # 13 input neurons (features), one hidden layer with 20 neurons, another hidden layer with 10 neurons, and 3 output neurons (for each class)\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "dnn = DenseNeuralNetwork(layers=layers, learning_rate=learning_rate, epochs=epochs)\n",
    "\n",
    "# Train the neural network\n",
    "dnn.fit(X_train.T, y_train.T)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = dnn.predict(X_test.T)\n",
    "\n",
    "# Convert one-hot encoded test labels back to original form\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test_labels) * 100\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
